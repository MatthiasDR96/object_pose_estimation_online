{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_object_detection.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1et3nAPyctxF8Cv9zSWkPeaGbn8yObjBP","authorship_tag":"ABX9TyNflDWBTK/4w/ntjhAZOs+D"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Kh6sLc5zoj3K"},"source":["# Object detectie\n","\n","\n","> We hebben nu de baselink-camera transformatie, we kunnen dus punten gezien vanuit de camera projecteren in baselink frame zodat de robot de positie van het object kent. In dit script zal je een foto van een object in het robot veld inladen waarna je hierop enkele image processing stappen uitvoert zodat je de positie van het object in pixelcoördinaten kan bepalen. Deze kan je met de intrinsieke matrix omzetten naar metrische coördinaten in camera frame dewelke je vervolgens met de baselink-camera transformatie kan omzetten in robot frame. "]},{"cell_type":"markdown","metadata":{"id":"8uZWow4_pM96"},"source":["# 1. Importeer bibliotheken\n","\n","Deze bibliotheken zijn nodig voor het volbrengen van dit script. In de CameraCalibration bibliotheek (die je vindt in de Classes folder op je Google Drive) vind je de code achter de commando's die je hier zal gebruiken. "]},{"cell_type":"code","metadata":{"id":"UfN5S2QdxD7V"},"source":["import sys\n","sys.path.append('/content/drive/My Drive/object_pose_estimation_online')\n","from Classes.CameraCalibration import *\n","from google.colab.patches import cv2_imshow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqN3W_l6pTIx"},"source":["\n","# 2. Importeer alle beelden voor object detectie\n","\n","> De afbeelding van het object in het robotveld vind je onder de map /content/drive/My Drive/object_pose_estimation_online/data/object_detection_images. Als je dit uitprint kan je zien dat er een padnaam aanwezig is naar een afbeelding met een ronde vorm. \n","\n"]},{"cell_type":"code","metadata":{"id":"aZcEqVWOpj_K"},"source":["# Get all image's path names\n","object_detection_images_file = '/content/drive/My Drive/object_pose_estimation_online/data/object_detection_images/*.png'\n","images_path_names = get_image_path_names(object_detection_images_file)\n","print(images_path_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNVazXkeqG1T"},"source":["> We kunnen terug kijken hoe een dergelijke afbeelding eruit ziet. "]},{"cell_type":"code","metadata":{"id":"x5Cs2AJFxMEb"},"source":["# Preview image\n","image_path = images_path_names[0]\n","image = cv2.imread(image_path)\n","cv2_imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6tJ2aPJYqdzr"},"source":["# 3. Image processing\n","\n","\n","\n","> In de volgende stappen zal je enkele image processing stappen uitvoeren die het ruwe RGB beeld zullen verwerken naar de uiteindelijke positie van het object in pixelcoördinaten. We zetten het beeld eerst om naar grayscale om de grote hoeveelheid data te reduceren. Kleur informatie is in deze toepassing namelijk niet van belang. \n","\n"]},{"cell_type":"code","metadata":{"id":"FrbQ0Vq_xMG0"},"source":["# To grayscale\n","gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","cv2_imshow(gray_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ic7QVjtJq7Oh"},"source":["\n","\n","> Vervolgens filteren we het beeld met een 7x7 Gaussiaanse filter om de hoogfrequente ruis eruit te krijgen. Dit zal ons helpen bij het detecteren van 'edges' in het beeld.\n","\n"]},{"cell_type":"code","metadata":{"id":"yiZbGz_fj2T7"},"source":["# Gaussian blur\n","blurred_image = cv2.GaussianBlur(gray_image, (7, 7), 0)\n","cv2_imshow(blurred_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNN2xtj2rS_Y"},"source":["\n","\n","> Hier komt de stap waarbij we in de geblurde afbeelding randen zullen detecteren. Het gebruikte 'Canny Edge' algoritme zal pixels detecteren met een hoge intensiteitsgradiënt (groot verschil in pixelwaardes tussen twee opeenvolgende pixels). Als output krijgen we een binair (enkel 0 en 255 waarden) beeld met alle randen.\n","\n"]},{"cell_type":"code","metadata":{"id":"fbhc8Sh9mpqy"},"source":["# Canny Edge detection\n","canny = cv2.Canny(blurred_image, 50, 200)\n","cv2_imshow(canny)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuV4C9kirtES"},"source":["\n","\n","> Je kan zien dat er links en rechts wat dingen mee gedetecteerd worden die we niet in ons resultaat willen. We zullen het beeld dus knippen. We definiëren met andere woorden een 'Region of Interest' (ROI) met enkel de ruimte die voor ons interessant is, namelijk het robot veld. We kijken eerst naar het aantal pixels in u- en v- richting dat de afbeelding heeft, en daarop baseren we ons om de pixels buiten de ROI op 0 te zetten (zwart).\n","\n"]},{"cell_type":"code","metadata":{"id":"UJavjbd3sJjr"},"source":["# Shape of image in u- and v- pixels\n","print(\"Image shape: \" + str(canny.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pn5gdSDCnfXd"},"source":["# Define ROI\n","canny[:, 0:450] = 0  # All pixels from u-value 0 till 450 are put to zero for all v\n","canny[:, 900:1280] = 0  # All pixels from u-value 900 till 1280 are put to zero for all v\n","roi = canny\n","cv2_imshow(roi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjMn_apCsX94"},"source":["\n","\n","> Als volgt gaan we de gedetecteerde randen die slechts 1 pixel breed zijn 'dilaten' (zoek op) om de eventuele gaten die in de rand zouden voorkomen te dichten. \n","\n"]},{"cell_type":"code","metadata":{"id":"SKbY90-6kRSb"},"source":["# Dilate image\n","roi_d = cv2.dilate(roi, None, iterations=2)\n","cv2_imshow(roi_d)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJYP12Ifsou4"},"source":["\n","\n","> Vervolgens 'eroderen' (zoek op) we de afbeelding terug om de rand weer te verkleinen. Maar de gaten blijven zo wel opgevuld zodat we zeker zijn van een continue rand. \n","\n"]},{"cell_type":"code","metadata":{"id":"wTYV4RE8so03"},"source":["# Erode image\n","roi_e = cv2.erode(roi_d, None, iterations=2)\n","cv2_imshow(roi_e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omi9utYHs9vP"},"source":["# 4. Contour detection\n","\n","\n","\n","> Nu we de image processing hebben beïndigd kunnen we overgaan tot het detecteren van 'contouren' (zoek op) in de afbeelding. Deze kunnen worden gevisualiseerd op de afbeelding. \n","\n"]},{"cell_type":"code","metadata":{"id":"PNOlYNw0xPHO"},"source":["# Find contours\n","contours = cv2.findContours(roi_e, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","contours = imutils.grab_contours(contours)\n","image_with_contours = cv2.drawContours(image.copy(), contours, -1, (0, 255, 0), 3)\n","cv2_imshow(image_with_contours)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghpq3g81utfO"},"source":["\n","\n","> We kunnen rond deze contour een bounding box definiëren. \n","\n"]},{"cell_type":"code","metadata":{"id":"j9h-MKtZm4FM"},"source":["# Compute the bounding box of the contour\n","box = cv2.minAreaRect(contours[0])\n","box = cv2.cv.BoxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n","image_with_box = cv2.drawContours(image.copy(), [box.astype(\"int\")], -1, (0, 255, 0), 2)\n","cv2_imshow(image_with_box)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zetj5fvNvIl7"},"source":["\n","\n","> Als we van de vier hoekpunten van deze bounding box gedefiniëerd in pixelcoördinaten de gemiddelde coördinaat nemen, dan bekomen we het centerpunt van ons object in pixelcoördinaten. \n","\n"]},{"cell_type":"code","metadata":{"id":"5lIPCrJ3ucEZ"},"source":["# Print box coördinates\n","print(\"Coördinaten van de bounding box in pixel coördinaten (u,v): \\n\\n\" + str(box))\n","mean_coordinate = np.mean(box, axis=0)\n","print(\"\\nCenterpunt van de box in pixel coördinaten (u,v): \\n\\n\" + str(mean_coordinate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsbhnFSB6dQA"},"source":["\n","\n","> We kunnen dit visualiseren door een punt te projecteren op de afbeelding met deze pixel coördinaten. Dit punt zullen we nu projecteren in baselink frame.\n","\n"]},{"cell_type":"code","metadata":{"id":"K0k-w64n6tZT"},"source":["cv2.circle(image_with_box, tuple(mean_coordinate), 3, [255, 0, 0], thickness=3)\n","cv2_imshow(image_with_box)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QxBbZP_Rvkkf"},"source":["# 5. Definiëren van de baselink-target transformatie\n","\n","> We weten nu op welk coördinaat het object zich bevindt in pixel coördinaten. Dit dienen we nog om te zetten naar camera coördinaten en naar baselink coördinaten. Hiervoor laden we terug de eerder berekende intrinsieke matrix en de baselink-camera matrix.\n","\n"]},{"cell_type":"code","metadata":{"id":"2JwmkOwOvqR4"},"source":["# Load intrinsic camera matrix and distortion coefficients\n","intrinsic_camera_matrix_file = '/content/drive/My Drive/object_pose_estimation_online/data/matrix_files/intrinsic_camera_properties.npz'\n","mtx, dist = load_intrinsic_camera_matrix(intrinsic_camera_matrix_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJKhYngEvqP1"},"source":["# Baselink-camera matrix\n","baselink_camera_transformation_file = '/content/drive/My Drive/object_pose_estimation_online/data/matrix_files/mean_baselink_camera_transformation.npy'\n","bc_transform = get_numpy_data(baselink_camera_transformation_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpUapj302cc2"},"source":["\n","\n","> Als we nog even de volledige intrinsieke matrix bekijken: \n","\n","$$\\begin{bmatrix} u'\\\\ v' \\\\ w' \\end{bmatrix} = K *\\begin{bmatrix} X\\\\ Y \\\\ Z \\end{bmatrix}$$ \n","\n","> Dan zien we dat coördinaten $[X, Y, Z]$ worden omgezet in $[u', v', w']$. Om van pixelcoördinaten naar metrische coördinaten in camera frame te gaan moeten we deze matrix nog inverteren.\n","\n","$$\\begin{bmatrix} X\\\\ Y \\\\ Z \\end{bmatrix} = K^{-1} *\\begin{bmatrix} u'\\\\ v' \\\\ w' \\end{bmatrix}$$ \n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"p_pYtiOn27Cw"},"source":["# Invert matrix\n","mtx_inv = invert_transform(mtx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyVHyD5l291l"},"source":["\n","\n","> We zien ook dat het niet u en v is dat we nodig hebben voor een goede omvorming, maar u', v' en w'. We hebben gezien dat $u' = u * w'$  ,   $v' = v * w'$  ,  en $w' = Z$. Om van 2D terug naar 3D te kunnen gaan hebben we dus nog de diepte nodig van het punt. M.a.w de afstand van de pixel tot het centerpunt van het object. Dit kunnen we niet uit RGB beelden halen. De camera in het labo is echter een RGBD camera, die ook diepte kan meten. Hieruit hebben we gehaald dat het centerpunt van het object (de pixel die we berekend hebben) op 0.745 meter ligt van de pixel. Met deze info kunnen we dus $[u', v', w']$ bekomen. \n","\n"]},{"cell_type":"code","metadata":{"id":"4j6GX5r0xP0T"},"source":["# Convert (u, v) to (u', v', w') using Z\n","Z = 0.745\n","u_accent = mean_coordinate[0] * Z\n","v_accent = mean_coordinate[1] * Z\n","w_accent = Z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VA9SPlGO4I1Y"},"source":["\n","\n","> Nu dat we het [u', v', w'] coördinaat hebben van het object, kunnen we de inverse van de intrinsieke matrix gebruiken om deze om te zetten naar een metrisch coördinaat in camera frame. \n","\n"]},{"cell_type":"code","metadata":{"id":"ojt3b5YPwZsb"},"source":["# Calculate position in camera frame\n","pixel_coordinate_accent = [u_accent, v_accent, w_accent]\n","c_p = np.dot(invert_transform(mtx), pixel_coordinate_accent)\n","c_p = np.append (c_p, [1]) # To make it a 4x1 vector\n","print(\"Coördinaat in camera frame: \\n\\n\" + str(np.reshape(c_p, (4, 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pawhrgwc4vyG"},"source":["\n","\n","> Tot slot projecteren we dit nog naar baselink frame met de baselink-camera transformatie. \n","\n"]},{"cell_type":"code","metadata":{"id":"fODvrS09x7oy"},"source":["# Covert position to baselink frame\n","b_p = np.dot(bc_transform,  c_p)\n","print(\"Coördinaat in baselink frame: \\n\\n\" + str(np.reshape(b_p, (4, 1))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GPuFhEuA1hbE"},"source":["\n","\n","> Dit is dus het centerpunt van het object gedefiniëerd in het baselink frame van de robot. Om de robot naar een positie te kunnen sturen hebben we niet enkel een positie nodig maar hebben we een volledige transformatiematrix nodig van baselink naar target. Hieronder definiëren we nog welke oriëntatie we bij het object willen hebben door custom een 3x3 rotatiematrix te definiëren. Vervolgens brengen we die custom rotatiematrix en de berekende translatie matrix (positie) samen tot een volwaardige transformatiematrix. \n","\n"]},{"cell_type":"code","metadata":{"id":"5XEB7QZiys3Q"},"source":["# Create target orientation (custom)\n","R_target = np.array([[math.sqrt(2)/2, -math.sqrt(2)/2, 0],\n","       [-math.sqrt(2)/2, -math.sqrt(2)/2, 0],\n","       [0, 0, -1]])\n","print(\"Rotatie matrix: \\n\\n\" + str(R_target))\n","\n","# Create target position (calculated from image)\n","P_target = b_p\n","print(\"\\nTranslatie matrix: \\n\\n\" + str(np.reshape(P_target, (4, 1))))\n","\n","# Create total transformation (R and T)\n","bt_transform = np.zeros((4, 4))\n","bt_transform[:3, :3] = R_target\n","bt_transform[:, 3] = P_target\n","print(\"\\nTotale transformatiematrix: \\n\\n\" + str(bt_transform))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nq7Ixa322EiX"},"source":["\n","\n","> Dit is wat we wilden bekomen. We kunnen nu als aller laatste stap deze matrix opslaan en deze in het volgende script visualiseren en de robot ernaar toe laten bewegen. \n","\n"]},{"cell_type":"code","metadata":{"id":"RUartGq5zKuY"},"source":["# Save the baselink - target transformation matrix\n","baselink_target_transformation_file = '/content/drive/My Drive/object_pose_estimation_online/data/matrix_files/baselink_target_transformation'\n","save_to_numpy(baselink_target_transformation_file, bt_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t40GoPEmGVjL"},"source":["*  VRAAG 1: Wat is 'dilaten'? En wat is het nut ervan?\n","*  VRAAG 2: Wat is 'eroderen'? En wat is het nut ervan?\n","*  VRAAG 3: Wat is een 'contour'? En wat is het nut ervan?\n","*  VRAAG 4: Wat gebeurt er met de pixelcoördinaten van het punt als de Z-coördinaat groter wordt?\n"]}]}